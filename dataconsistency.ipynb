{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73d8e087",
   "metadata": {},
   "source": [
    "DATA CLEANER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc42036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load dataset\n",
    "df = pd.read_csv(\"nyc_traffic_raw.csv\")   # change filename to yours\n",
    "\n",
    "# 2. Keep only useful columns\n",
    "df = df[[\"LINK_ID\", \"BOROUGH\", \"DATA_AS_OF\", \"SPEED\", \"TRAVEL_TIME\", \"LINK_NAME\"]]\n",
    "\n",
    "# 3. Clean timestamp\n",
    "df[\"DATA_AS_OF\"] = pd.to_datetime(df[\"DATA_AS_OF\"])\n",
    "\n",
    "# 4. Sort by road & time\n",
    "df = df.sort_values(by=[\"LINK_ID\", \"DATA_AS_OF\"])\n",
    "\n",
    "# 5. Create time features\n",
    "df[\"hour\"] = df[\"DATA_AS_OF\"].dt.hour\n",
    "df[\"day_of_week\"] = df[\"DATA_AS_OF\"].dt.dayofweek   # 0=Mon, 6=Sun\n",
    "df[\"date\"] = df[\"DATA_AS_OF\"].dt.date\n",
    "\n",
    "# 6. Generate lag features (last 12h speeds per road)\n",
    "for lag in range(1, 13):\n",
    "    df[f\"lag_{lag}h\"] = df.groupby(\"LINK_ID\")[\"SPEED\"].shift(lag)\n",
    "\n",
    "# 7. Create congestion ratio\n",
    "# (using max observed speed per road as \"free flow\" approximation)\n",
    "df[\"free_flow_speed\"] = df.groupby(\"LINK_ID\")[\"SPEED\"].transform(\"max\")\n",
    "df[\"congestion_ratio\"] = df[\"SPEED\"] / df[\"free_flow_speed\"]\n",
    "\n",
    "# 8. Create classification labels\n",
    "def label_traffic(ratio):\n",
    "    if ratio > 0.8:\n",
    "        return \"Light\"\n",
    "    elif ratio > 0.5:\n",
    "        return \"Moderate\"\n",
    "    else:\n",
    "        return \"Heavy\"\n",
    "\n",
    "df[\"traffic_label\"] = df[\"congestion_ratio\"].apply(label_traffic)\n",
    "\n",
    "# 9. Drop rows with missing values (from lags)\n",
    "df = df.dropna()\n",
    "\n",
    "# 10. Save cleaned dataset\n",
    "df.to_csv(\"nyc_traffic_cleaned.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Preprocessing complete. Clean dataset saved as nyc_traffic_cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ff0222",
   "metadata": {},
   "source": [
    "VALIDATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56200226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your cleaned dataset\n",
    "\n",
    "df = pd.read_csv(\"nyc_traffic_fully_cleaned.csv\")\n",
    "print(\"üîç Starting dataset validation...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Time continuity check\n",
    "print(\"\\n‚è∞ Checking timestamp continuity...\")\n",
    "df[\"DATA_AS_OF\"] = pd.to_datetime(df[\"DATA_AS_OF\"])\n",
    "time_gaps = df.groupby(\"LINK_ID\")[\"DATA_AS_OF\"].diff().value_counts().head(10)\n",
    "print(\"Most common time gaps:\\n\", time_gaps)\n",
    "\n",
    "# 2. Speed sanity check\n",
    "print(\"\\nüö¶ Checking SPEED values...\")\n",
    "print(df[\"SPEED\"].describe())\n",
    "if (df[\"SPEED\"] < 0).any():\n",
    "    print(\"‚ö†Ô∏è Warning: Negative speeds found!\")\n",
    "if (df[\"SPEED\"] > 200).any():\n",
    "    print(\"‚ö†Ô∏è Warning: Extremely high speeds found (>200 km/h).\")\n",
    "\n",
    "# 3. Travel time consistency\n",
    "print(\"\\n‚è≥ Checking TRAVEL_TIME values...\")\n",
    "print(df[\"TRAVEL_TIME\"].describe())\n",
    "if (df[\"TRAVEL_TIME\"] <= 0).any():\n",
    "    print(\"‚ö†Ô∏è Warning: Non-positive travel times found!\")\n",
    "\n",
    "# 4. Congestion ratio range\n",
    "print(\"\\nüìâ Checking congestion ratio...\")\n",
    "if \"congestion_ratio\" in df.columns:\n",
    "    print(df[\"congestion_ratio\"].describe())\n",
    "    if (df[\"congestion_ratio\"] < 0).any() or (df[\"congestion_ratio\"] > 1).any():\n",
    "        print(\"‚ö†Ô∏è Warning: Congestion ratio out of [0,1] range!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è congestion_ratio column not found. Did you run preprocessing?\")\n",
    "\n",
    "# 5. Label balance check\n",
    "print(\"\\nüìä Checking traffic label balance...\")\n",
    "if \"traffic_label\" in df.columns:\n",
    "    print(df[\"traffic_label\"].value_counts(normalize=True))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è traffic_label column not found. Did you run preprocessing?\")\n",
    "\n",
    "# 6. Duplicate row check\n",
    "print(\"\\nüìë Checking duplicate rows...\")\n",
    "duplicates = df.duplicated(subset=[\"LINK_ID\", \"DATA_AS_OF\"]).sum()\n",
    "print(f\"Found {duplicates} duplicate rows.\")\n",
    "\n",
    "# 7. Borough consistency check\n",
    "print(\"\\nüèôÔ∏è Checking borough consistency...\")\n",
    "borough_consistency = df.groupby(\"LINK_ID\")[\"BOROUGH\"].nunique().value_counts()\n",
    "print(\"Boroughs per road segment:\\n\", borough_consistency)\n",
    "\n",
    "print(\"\\n‚úÖ Validation complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407f40b4",
   "metadata": {},
   "source": [
    "VALIDATION INCONSISTENCY CLEANER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824a756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load dataset\n",
    "# -----------------------------\n",
    "df = pd.read_csv(\"nyc_traffic_cleaned.csv\")  # replace with your file\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Remove invalid travel times\n",
    "# -----------------------------\n",
    "df = df[df[\"TRAVEL_TIME\"] > 0]\n",
    "\n",
    "# Optional: remove extreme travel times (> mean + 3*std)\n",
    "upper_limit = df[\"TRAVEL_TIME\"].mean() + 3 * df[\"TRAVEL_TIME\"].std()\n",
    "df = df[df[\"TRAVEL_TIME\"] <= upper_limit]\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Round timestamps (optional)\n",
    "# -----------------------------\n",
    "df[\"DATA_AS_OF\"] = pd.to_datetime(df[\"DATA_AS_OF\"]).dt.round(\"5min\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Cap congestion ratio at 1 (already mostly clean, just in case)\n",
    "# -----------------------------\n",
    "if \"congestion_ratio\" in df.columns:\n",
    "    df[\"congestion_ratio\"] = df[\"congestion_ratio\"].clip(upper=1)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Optional: check SPEED sanity (no changes needed based on validation)\n",
    "# -----------------------------\n",
    "df = df[df[\"SPEED\"] >= 0]  # remove negative speeds if any\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Save cleaned dataset\n",
    "# -----------------------------\n",
    "df.to_csv(\"nyc_traffic_fully_cleaned.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Dataset fully cleaned and saved (CSV + Parquet). Ready for ML training.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0eb497",
   "metadata": {},
   "source": [
    "HOT ENCODING OF PLACE NAMES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa154589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ One-hot encoding done for BOROUGH only.\n",
      "‚û°Ô∏è Saved as nyc_traffic_encoded.csv\n",
      "Shape after: (9614098, 27)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load cleaned dataset\n",
    "# -----------------------------\n",
    "df = pd.read_csv(\"nyc_traffic_fully_cleaned.csv\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Only one-hot encode BOROUGH\n",
    "# -----------------------------\n",
    "if \"BOROUGH\" in df.columns:\n",
    "    df = pd.get_dummies(df, columns=[\"BOROUGH\"], drop_first=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Save new dataset\n",
    "# -----------------------------\n",
    "df.to_csv(\"nyc_traffic_encoded.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ One-hot encoding done for BOROUGH only.\")\n",
    "print(\"‚û°Ô∏è Saved as nyc_traffic_encoded.csv\")\n",
    "print(\"Shape after:\", df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a8fbb0",
   "metadata": {},
   "source": [
    "boolean to integer converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6bd00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Borough columns converted to 0/1 successfully!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"nyc_traffic_encoded.csv\")\n",
    "\n",
    "# List of borough boolean columns\n",
    "borough_cols = ['BOROUGH_Brooklyn', 'BOROUGH_Manhattan', 'BOROUGH_Queens', 'BOROUGH_Staten Island']\n",
    "\n",
    "# Convert these boolean columns to integers\n",
    "df[borough_cols] = df[borough_cols].astype(int)\n",
    "\n",
    "# Optional: save the updated dataset\n",
    "df.to_csv(\"your_dataset_int.csv\", index=False)\n",
    "\n",
    "print(\"Borough columns converted to 0/1 successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9435fca7",
   "metadata": {},
   "source": [
    "ADDING FUTURE LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a871b430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset transformed successfully! Saved to nyc_traffic_processed.csv\n",
      "‚úÖ Dataset transformed successfully! Saved to nyc_traffic_processed.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load dataset\n",
    "# -----------------------------\n",
    "df = pd.read_csv(\"your_dataset_int.csv\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Sort by time\n",
    "# -----------------------------\n",
    "df[\"DATA_AS_OF\"] = pd.to_datetime(df[\"DATA_AS_OF\"])\n",
    "df = df.sort_values([\"LINK_ID\", \"DATA_AS_OF\"]).reset_index(drop=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Create FUTURE label\n",
    "# -----------------------------\n",
    "# Assuming your data is 5-min interval ‚Üí 12 steps = 1 hour\n",
    "df[\"future_label\"] = df.groupby(\"LINK_ID\")[\"traffic_label\"].shift(-12)\n",
    "\n",
    "# Drop rows where future_label is NaN (at the tail of each group)\n",
    "df = df.dropna(subset=[\"future_label\"])\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Drop unnecessary columns\n",
    "# -----------------------------\n",
    "drop_cols = [\"LINK_NAME\", \"date\", \"DATA_AS_OF\"]  # not useful for model directly\n",
    "df = df.drop(columns=drop_cols)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Save processed dataset\n",
    "# -----------------------------\n",
    "df.to_csv(\"nyc_traffic_processed.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Dataset transformed successfully! Saved to nyc_traffic_processed.csv\")\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load dataset\n",
    "# -----------------------------\n",
    "df = pd.read_csv(\"nyc_traffic_encoded.csv\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Sort by time\n",
    "# -----------------------------\n",
    "df[\"DATA_AS_OF\"] = pd.to_datetime(df[\"DATA_AS_OF\"])\n",
    "df = df.sort_values([\"LINK_ID\", \"DATA_AS_OF\"]).reset_index(drop=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Create FUTURE label\n",
    "# -----------------------------\n",
    "# Assuming your data is 5-min interval ‚Üí 12 steps = 1 hour\n",
    "df[\"future_label\"] = df.groupby(\"LINK_ID\")[\"traffic_label\"].shift(-12)\n",
    "\n",
    "# Drop rows where future_label is NaN (at the tail of each group)\n",
    "df = df.dropna(subset=[\"future_label\"])\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Drop unnecessary columns\n",
    "# -----------------------------\n",
    "drop_cols = [\"LINK_NAME\", \"date\", \"DATA_AS_OF\"]  # not useful for model directly\n",
    "df = df.drop(columns=drop_cols)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Save processed dataset\n",
    "# -----------------------------\n",
    "df.to_csv(\"final.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Dataset transformed successfully! Saved to nyc_traffic_processed.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4733889a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current labels:\n",
      "traffic_label\n",
      "Heavy       4830646\n",
      "Moderate    4312592\n",
      "Light        469516\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Future labels (1h later):\n",
      "future_label\n",
      "Heavy       4830912\n",
      "Moderate    4312401\n",
      "Light        469441\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Current labels:\")\n",
    "print(df[\"traffic_label\"].value_counts())\n",
    "\n",
    "print(\"\\nFuture labels (1h later):\")\n",
    "print(df[\"future_label\"].value_counts())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
